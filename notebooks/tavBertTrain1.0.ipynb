{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898b79f2-33c0-4dcd-9908-7f23b6fd294d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f1f3828f-a073-47d1-9d8e-18fb259b2472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import datasets\n",
    "import transformers\n",
    "from transformers import AutoTokenizer,AutoModelForMaskedLM\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fd722330-aa59-4111-8bcf-9e59b9ddc844",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "601f21ab-df81-425d-b90a-2206528a3ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_csv('EmbibleDataset/train_df.csv', sep='\\\\t', encoding='utf-8')\n",
    "valid_df=pd.read_csv('EmbibleDataset/valid_df.csv', sep='\\\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1b856dea-f6d6-4d11-96bd-11a7d5da20c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=train_df.drop(columns=[\"verse_idx.1\",\"verse.1\",\"name\"])\n",
    "#train_df[\"name.1\"].rename(\"name\")\n",
    "train_df.rename(columns={\"name.1\": 'name'}, inplace=True)\n",
    "#valid_df=train_df.drop(columns=[\"verse_idx.1\",\"verse.1\",\"name\"])\n",
    "#train_df[\"name.1\"].rename(\"name\")\n",
    "#valid_df.rename(columns={\"name.1\": 'name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9b42299c-5c5e-40fb-a83a-a2dc8075f2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      verse_idx   verse                                               name\n",
      "0        Isaiah     0.0  חֲזוֹן֙ יְשַֽׁעְיָ֣הוּ בֶן אָמ֔וֹץ אֲשֶׁ֣ר חָז...\n",
      "1        Isaiah     1.0  שִׁמְע֤וּ שָׁמַ֙יִם֙ וְהַאֲזִ֣ינִי אֶ֔רֶץ כִּ֥...\n",
      "2        Isaiah     2.0  יָדַ֥ע שׁוֹר֙ קֹנֵ֔הוּ וַחֲמ֖וֹר אֵב֣וּס בְּעָ...\n",
      "3        Isaiah     3.0  ה֣וֹי גּ֣וֹי חֹטֵ֗א עַ֚ם כֶּ֣בֶד עָוֺ֔ן זֶ֣רַע...\n",
      "4        Isaiah     4.0  עַ֣ל מֶ֥ה תֻכּ֛וּ ע֖וֹד תּוֹסִ֣יפוּ סָרָ֑ה כָּ...\n",
      "...         ...     ...                                                ...\n",
      "22145    Exodus  1208.0  וַיְכַ֥ס הֶעָנָ֖ן אֶת אֹ֣הֶל מוֹעֵ֑ד וּכְב֣וֹד...\n",
      "22146    Exodus  1209.0  וְלֹא יָכֹ֣ל מֹשֶׁ֗ה לָבוֹא֙ אֶל אֹ֣הֶל מוֹעֵ֔...\n",
      "22147    Exodus  1210.0  וּבְהֵעָל֤וֹת הֶֽעָנָן֙ מֵעַ֣ל הַמִּשְׁכָּ֔ן י...\n",
      "22148    Exodus  1211.0  וְאִם לֹ֥א יֵעָלֶ֖ה הֶעָנָ֑ן וְלֹ֣א יִסְע֔וּ ע...\n",
      "22149    Exodus  1212.0  כִּי֩ עֲנַ֨ן יְהוָ֤ה עַֽל הַמִּשְׁכָּן֙ יוֹמָ֔...\n",
      "\n",
      "[22150 rows x 3 columns]\n",
      "         name  verse_idx                                              verse\n",
      "294    Isaiah      294.0  הָכִ֧ינוּ לְבָנָ֛יו מַטְבֵּ֖חַ בַּעֲוֺ֣ן אֲבוֹ...\n",
      "1252   Isaiah     1252.0  וְאַתֶּם֙ עֹזְבֵ֣י יְהוָ֔ה הַשְּׁכֵחִ֖ים אֶת ה...\n",
      "279    Isaiah      279.0  מַכֶּ֤ה עַמִּים֙ בְּעֶבְרָ֔ה מַכַּ֖ת בִּלְתִּ֣...\n",
      "541    Isaiah      541.0  וְחָנִ֥יתִי כַדּ֖וּר עָלָ֑יִךְ וְצַרְתִּ֤י עָל...\n",
      "619    Isaiah      619.0  עַד יֵ֨עָרֶ֥ה עָלֵ֛ינוּ ר֖וּחַ מִמָּר֑וֹם וְהָ...\n",
      "...       ...        ...                                                ...\n",
      "22964  Exodus      955.0  וַיֹּ֖אמֶר אֵלָ֑יו אִם אֵ֤ין פָּנֶ֙יךָ֙ הֹלְכִ...\n",
      "22263  Exodus      254.0  וַיֹּ֣אמֶר אֲלֵהֶ֗ם יְהִ֨י כֵ֤ן יְהוָה֙ עִמָּכ...\n",
      "23156  Exodus     1147.0  וַֽיַּעֲשׂ֗וּ שְׁתֵּי֙ מִשְׁבְּצֹ֣ת זָהָ֔ב וּש...\n",
      "22519  Exodus      510.0  וַיּוֹצֵ֨א מֹשֶׁ֧ה אֶת הָעָ֛ם לִקְרַ֥את הָֽאֱל...\n",
      "23207  Exodus     1198.0  וַיָּ֤שֶׂם אֶת הַמְּנֹרָה֙ בְּאֹ֣הֶל מוֹעֵ֔ד נ...\n",
      "\n",
      "[536 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_df)\n",
    "print(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0fc7ee08-7ad8-4393-8c11-bdf652527560",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/fono/.cache/huggingface/hub/models--tau--tavbert-he/snapshots/41265b09a862144b2517afdfd46da4388f1380df/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"tau/tavbert-he\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 2050,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 345\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/fono/.cache/huggingface/hub/models--tau--tavbert-he/snapshots/41265b09a862144b2517afdfd46da4388f1380df/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at tau/tavbert-he.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "loading file tokenizer.json from cache at /home/fono/.cache/huggingface/hub/models--tau--tavbert-he/snapshots/41265b09a862144b2517afdfd46da4388f1380df/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/fono/.cache/huggingface/hub/models--tau--tavbert-he/snapshots/41265b09a862144b2517afdfd46da4388f1380df/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/fono/.cache/huggingface/hub/models--tau--tavbert-he/snapshots/41265b09a862144b2517afdfd46da4388f1380df/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "cp = \"tau/tavbert-he\"\n",
    "model=AutoModelForMaskedLM.from_pretrained(cp)\n",
    "tokenizer=AutoTokenizer.from_pretrained(cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "06406f33-d5a4-4426-9615-4110819fc44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds=datasets.Dataset.from_pandas(train_df)\n",
    "valid_ds=datasets.Dataset.from_pandas(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0eaf4e82-d2b5-431d-b28a-756750c61dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12475f9451c34b5883c2226b8dfd2873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22150 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c24aa27d9044f4085f27639fa9c5a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/536 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def tokenize_function(dataset):\n",
    "  return tokenizer(str(dataset[\"verse\"]))\n",
    "tokenized_train_ds=train_ds.map(tokenize_function)\n",
    "tokenized_valid_ds=valid_ds.map(tokenize_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fa4d01f7-ef3f-4498-8cb6-4cd2afaaee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "984b3216-39f1-42b5-b142-b41895e8eb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(tokenized_train_ds)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"EmbibleModels/Embibert-finetuned-tavbert-10epochs\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=False,\n",
    "    logging_steps=logging_steps,\n",
    "    num_train_epochs=10\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "641f6ebb-61ff-4155-b1a8-ddfc9af924d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import f1_score\n",
    "\n",
    "# def compute_metrics(pred):\n",
    "#   return {'f1:':pred}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f2d033a6-8ae9-48d7-8c9a-f485247e2ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_train_ds,\n",
    "    eval_dataset=tokenized_valid_ds,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8832463b-1443-40de-a72a-ded6b6993649",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, verse, name. If verse_idx, verse, name are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 22150\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3470\n",
      "  Number of trainable parameters = 87489369\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3470' max='3470' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3470/3470 09:36, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.334800</td>\n",
       "      <td>6.708065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.198900</td>\n",
       "      <td>9.746973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.183300</td>\n",
       "      <td>12.123498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.185900</td>\n",
       "      <td>11.782933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.172900</td>\n",
       "      <td>12.033704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.166000</td>\n",
       "      <td>12.216064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.164700</td>\n",
       "      <td>13.117076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.158200</td>\n",
       "      <td>12.655667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.176300</td>\n",
       "      <td>12.528611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.168000</td>\n",
       "      <td>12.847200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-10epochs/checkpoint-500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-10epochs/checkpoint-500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-10epochs/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-10epochs/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-10epochs/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-10epochs/checkpoint-1000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-10epochs/checkpoint-1000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-10epochs/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-10epochs/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-10epochs/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-10epochs/checkpoint-1500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-10epochs/checkpoint-1500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-10epochs/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-10epochs/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-10epochs/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-10epochs/checkpoint-2000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-10epochs/checkpoint-2000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-10epochs/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-10epochs/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-10epochs/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-10epochs/checkpoint-2500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-10epochs/checkpoint-2500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-10epochs/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-10epochs/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-10epochs/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-10epochs/checkpoint-3000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-10epochs/checkpoint-3000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-10epochs/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-10epochs/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-10epochs/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3470, training_loss=1.190897837702067, metrics={'train_runtime': 576.2079, 'train_samples_per_second': 384.41, 'train_steps_per_second': 6.022, 'total_flos': 910566259197228.0, 'train_loss': 1.190897837702067, 'epoch': 10.0})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a28e2523-e99b-4d76-b772-ec8d4a17923b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-10epochs\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-10epochs/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-10epochs/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-10epochs/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-10epochs/special_tokens_map.json\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-10epochs/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-10epochs/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"EmbibleModels/Embibert-finetuned-tavbert-10epochs\")\n",
    "model.save_pretrained(\"EmbibleModels/Embibert-finetuned-tavbert-10epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "45eab85b-1bad-4b80-8cbc-bb08b3d8f2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/fono/.cache/huggingface/hub/models--tau--tavbert-he/snapshots/41265b09a862144b2517afdfd46da4388f1380df/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"tau/tavbert-he\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 2050,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 345\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/fono/.cache/huggingface/hub/models--tau--tavbert-he/snapshots/41265b09a862144b2517afdfd46da4388f1380df/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at tau/tavbert-he.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "loading file tokenizer.json from cache at /home/fono/.cache/huggingface/hub/models--tau--tavbert-he/snapshots/41265b09a862144b2517afdfd46da4388f1380df/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/fono/.cache/huggingface/hub/models--tau--tavbert-he/snapshots/41265b09a862144b2517afdfd46da4388f1380df/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/fono/.cache/huggingface/hub/models--tau--tavbert-he/snapshots/41265b09a862144b2517afdfd46da4388f1380df/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "cp = \"tau/tavbert-he\"\n",
    "model=AutoModelForMaskedLM.from_pretrained(cp)\n",
    "tokenizer=AutoTokenizer.from_pretrained(cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bf27cf54-c402-4bad-90f3-35c85a5ab3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(tokenized_train_ds)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"EmbibleModels/Embibert-finetuned-tavbert-20epochs\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=False,\n",
    "    logging_steps=logging_steps,\n",
    "    num_train_epochs=20\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "335427dc-c4c4-43e1-a1ca-4ed15e85d315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b560918c-0629-458b-b472-d446ecb81e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, verse, name. If verse_idx, verse, name are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 22150\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6940\n",
      "  Number of trainable parameters = 87489369\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6940' max='6940' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6940/6940 16:36, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.318700</td>\n",
       "      <td>5.967810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.198700</td>\n",
       "      <td>9.797073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.182800</td>\n",
       "      <td>12.348807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.184700</td>\n",
       "      <td>12.969960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.174300</td>\n",
       "      <td>14.016263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.165200</td>\n",
       "      <td>13.316539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.167000</td>\n",
       "      <td>14.486106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.167800</td>\n",
       "      <td>14.302264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.179300</td>\n",
       "      <td>14.592799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.175300</td>\n",
       "      <td>14.134422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.157500</td>\n",
       "      <td>15.589350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.174000</td>\n",
       "      <td>15.590930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.171500</td>\n",
       "      <td>14.618776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.172600</td>\n",
       "      <td>15.377292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.176900</td>\n",
       "      <td>14.994708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.167500</td>\n",
       "      <td>15.110362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.170300</td>\n",
       "      <td>15.152133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.158000</td>\n",
       "      <td>15.542837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.165400</td>\n",
       "      <td>15.570130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.163800</td>\n",
       "      <td>15.510090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-1000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-1000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-1500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-1500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-2000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-2000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-2500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-2500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-3000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-3000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-3500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-3500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-3500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-4000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-4000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-4000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-4500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-4500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-4500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-5000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-5000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-5000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-5500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-5500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-5500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-6000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-6000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-6000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-6500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-6500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/checkpoint-6500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6940, training_loss=1.1795711550314076, metrics={'train_runtime': 996.3567, 'train_samples_per_second': 444.62, 'train_steps_per_second': 6.965, 'total_flos': 1821174657443172.0, 'train_loss': 1.1795711550314076, 'epoch': 20.0})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_train_ds,\n",
    "    eval_dataset=tokenized_valid_ds,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0aabcc93-1279-4914-a110-7ba7731ad023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-20epochs\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/special_tokens_map.json\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-20epochs/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"EmbibleModels/Embibert-finetuned-tavbert-20epochs\")\n",
    "model.save_pretrained(\"EmbibleModels/Embibert-finetuned-tavbert-20epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f411f07a-1db8-4555-8804-1f943b7acae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/fono/.cache/huggingface/hub/models--tau--tavbert-he/snapshots/41265b09a862144b2517afdfd46da4388f1380df/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"tau/tavbert-he\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 2050,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 345\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/fono/.cache/huggingface/hub/models--tau--tavbert-he/snapshots/41265b09a862144b2517afdfd46da4388f1380df/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at tau/tavbert-he.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "loading file tokenizer.json from cache at /home/fono/.cache/huggingface/hub/models--tau--tavbert-he/snapshots/41265b09a862144b2517afdfd46da4388f1380df/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/fono/.cache/huggingface/hub/models--tau--tavbert-he/snapshots/41265b09a862144b2517afdfd46da4388f1380df/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/fono/.cache/huggingface/hub/models--tau--tavbert-he/snapshots/41265b09a862144b2517afdfd46da4388f1380df/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "cp = \"tau/tavbert-he\"\n",
    "model=AutoModelForMaskedLM.from_pretrained(cp)\n",
    "tokenizer=AutoTokenizer.from_pretrained(cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d9e96b6b-eff5-4542-abea-e3cc5ba61855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(tokenized_train_ds)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"EmbibleModels/Embibert-finetuned-tavbert-50epochs\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=False,\n",
    "    logging_steps=logging_steps,\n",
    "    num_train_epochs=50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "291da944-ddc9-4128-a2cb-2fd6bb02758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "11dfcc52-2df2-453b-8df7-924d11958675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, verse, name. If verse_idx, verse, name are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 22150\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 17350\n",
      "  Number of trainable parameters = 87489369\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17350' max='17350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17350/17350 40:32, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.390800</td>\n",
       "      <td>6.705500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.197300</td>\n",
       "      <td>9.663179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.184900</td>\n",
       "      <td>11.419830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.183000</td>\n",
       "      <td>11.838439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.176300</td>\n",
       "      <td>13.436655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.167800</td>\n",
       "      <td>12.881771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.168200</td>\n",
       "      <td>13.896580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.160500</td>\n",
       "      <td>14.877946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.181700</td>\n",
       "      <td>13.739940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.171400</td>\n",
       "      <td>14.347319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.159300</td>\n",
       "      <td>13.314993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.179400</td>\n",
       "      <td>14.589144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.173200</td>\n",
       "      <td>14.144187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.182400</td>\n",
       "      <td>15.470383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.178800</td>\n",
       "      <td>14.678509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.171600</td>\n",
       "      <td>16.133764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.172800</td>\n",
       "      <td>15.373765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.162600</td>\n",
       "      <td>16.639425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.175000</td>\n",
       "      <td>15.718652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.167500</td>\n",
       "      <td>17.638475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.160200</td>\n",
       "      <td>16.576052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.164200</td>\n",
       "      <td>16.467340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.165100</td>\n",
       "      <td>16.115410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.172900</td>\n",
       "      <td>16.213285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.157900</td>\n",
       "      <td>16.623484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.174500</td>\n",
       "      <td>17.210106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.170000</td>\n",
       "      <td>17.541391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.172100</td>\n",
       "      <td>17.590555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.171500</td>\n",
       "      <td>17.079330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.173900</td>\n",
       "      <td>17.154757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.164400</td>\n",
       "      <td>17.124123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.158100</td>\n",
       "      <td>18.004375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.156300</td>\n",
       "      <td>17.409519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.153300</td>\n",
       "      <td>16.586258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.162900</td>\n",
       "      <td>16.226797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.164800</td>\n",
       "      <td>16.527853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.176200</td>\n",
       "      <td>17.042528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.171800</td>\n",
       "      <td>17.640020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.166900</td>\n",
       "      <td>16.982155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.175000</td>\n",
       "      <td>17.515381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.164000</td>\n",
       "      <td>16.876163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.169500</td>\n",
       "      <td>17.445328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.163300</td>\n",
       "      <td>17.331314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.157500</td>\n",
       "      <td>17.170759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.156400</td>\n",
       "      <td>17.129333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.177600</td>\n",
       "      <td>17.007206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.159800</td>\n",
       "      <td>16.990824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.155900</td>\n",
       "      <td>17.054247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.168800</td>\n",
       "      <td>17.209829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.143800</td>\n",
       "      <td>17.297800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-1000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-1000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-1500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-1500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-2000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-2000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-2500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-2500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-3000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-3000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-3500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-3500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-3500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-4000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-4000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-4000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-4500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-4500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-4500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-5000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-5000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-5000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-5500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-5500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-5500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-6000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-6000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-6000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-6500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-6500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-6500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-7000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-7000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-7000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-7500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-7500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-7500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-8000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-8000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-8000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-8500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-8500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-8500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-9000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-9000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-9000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-9500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-9500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-9500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-10000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-10000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-10000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-10500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-10500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-10500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-11000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-11000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-11000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-11500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-11500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-11500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-12000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-12000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-12000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-12500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-12500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-12500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-13000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-13000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-13000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-13500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-13500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-13500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-14000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-14000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-14000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-14500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-14500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-14500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-15000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-15000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-15000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-15500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-15500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-15500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-15500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-16000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-16000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-16000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-16500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-16500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-16500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-16500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-16500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-17000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-17000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-17000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-17000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/checkpoint-17000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: verse_idx, name, verse, __index_level_0__. If verse_idx, name, verse, __index_level_0__ are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=17350, training_loss=1.1730595463909401, metrics={'train_runtime': 2433.0261, 'train_samples_per_second': 455.194, 'train_steps_per_second': 7.131, 'total_flos': 4552990602145920.0, 'train_loss': 1.1730595463909401, 'epoch': 50.0})"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_train_ds,\n",
    "    eval_dataset=tokenized_valid_ds,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ac154950-443b-470b-8384-9b039fbb977e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-tavbert-50epochs\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/special_tokens_map.json\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-tavbert-50epochs/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"EmbibleModels/Embibert-finetuned-tavbert-50epochs\")\n",
    "model.save_pretrained(\"EmbibleModels/Embibert-finetuned-tavbert-50epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda9fc54-0ef8-4e81-90e1-e14cfb991870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a4a0ca-6f47-477d-a4db-255838e33c35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
