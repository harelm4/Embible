{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ca793c9-e351-4d62-92df-a97ff44f49a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in ./.local/lib/python3.7/site-packages (2.8.0)\n",
      "Requirement already satisfied: responses<0.19 in ./.local/lib/python3.7/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /storage/modules/packages/anaconda3/lib/python3.7/site-packages (from datasets) (1.21.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /storage/modules/packages/anaconda3/lib/python3.7/site-packages (from datasets) (2.27.1)\n",
      "Requirement already satisfied: xxhash in ./.local/lib/python3.7/site-packages (from datasets) (3.1.0)\n",
      "Requirement already satisfied: multiprocess in ./.local/lib/python3.7/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /storage/modules/packages/anaconda3/lib/python3.7/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: packaging in /storage/modules/packages/anaconda3/lib/python3.7/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in ./.local/lib/python3.7/site-packages (from datasets) (10.0.1)\n",
      "Requirement already satisfied: pandas in /storage/modules/packages/anaconda3/lib/python3.7/site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /storage/modules/packages/anaconda3/lib/python3.7/site-packages (from datasets) (4.62.3)\n",
      "Requirement already satisfied: importlib-metadata in /storage/modules/packages/anaconda3/lib/python3.7/site-packages (from datasets) (4.8.2)\n",
      "Requirement already satisfied: aiohttp in ./.local/lib/python3.7/site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /storage/modules/packages/anaconda3/lib/python3.7/site-packages (from datasets) (2022.1.0)\n",
      "Requirement already satisfied: dill<0.3.7 in ./.local/lib/python3.7/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in ./.local/lib/python3.7/site-packages (from datasets) (0.11.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /storage/modules/packages/anaconda3/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.10.0.2)\n",
      "Requirement already satisfied: filelock in /storage/modules/packages/anaconda3/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.4.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /storage/modules/packages/anaconda3/lib/python3.7/site-packages (from packaging->datasets) (3.0.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /storage/modules/packages/anaconda3/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /storage/modules/packages/anaconda3/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /storage/modules/packages/anaconda3/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /storage/modules/packages/anaconda3/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.8)\n",
      "Requirement already satisfied: asynctest==0.13.0 in ./.local/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /storage/modules/packages/anaconda3/lib/python3.7/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.local/lib/python3.7/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.local/lib/python3.7/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.local/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.local/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./.local/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /storage/modules/packages/anaconda3/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.7.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /storage/modules/packages/anaconda3/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /storage/modules/packages/anaconda3/lib/python3.7/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /storage/modules/packages/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.7/site-packages (4.25.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./.local/lib/python3.7/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /storage/modules/packages/anaconda3/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: requests in /storage/modules/packages/anaconda3/lib/python3.7/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /storage/modules/packages/anaconda3/lib/python3.7/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /storage/modules/packages/anaconda3/lib/python3.7/site-packages (from transformers) (1.21.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /storage/modules/packages/anaconda3/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in ./.local/lib/python3.7/site-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /storage/modules/packages/anaconda3/lib/python3.7/site-packages (from transformers) (2021.11.2)\n",
      "Requirement already satisfied: filelock in /storage/modules/packages/anaconda3/lib/python3.7/site-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: importlib-metadata in /storage/modules/packages/anaconda3/lib/python3.7/site-packages (from transformers) (4.8.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /storage/modules/packages/anaconda3/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /storage/modules/packages/anaconda3/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /storage/modules/packages/anaconda3/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.7.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /storage/modules/packages/anaconda3/lib/python3.7/site-packages (from requests->transformers) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /storage/modules/packages/anaconda3/lib/python3.7/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /storage/modules/packages/anaconda3/lib/python3.7/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /storage/modules/packages/anaconda3/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.7/site-packages (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in /storage/modules/packages/anaconda3/lib/python3.7/site-packages (from torch) (3.10.0.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./.local/lib/python3.7/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./.local/lib/python3.7/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./.local/lib/python3.7/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./.local/lib/python3.7/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: wheel in /storage/modules/packages/anaconda3/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.37.1)\n",
      "Requirement already satisfied: setuptools in /storage/modules/packages/anaconda3/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (58.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71342536-129a-4a67-8f7b-8451dbaaa03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "851fcbd8-19a0-426a-96b0-4583f4ca4d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForMaskedLM\n",
    "cp = \"distilbert-base-multilingual-cased\"\n",
    "model=AutoModelForMaskedLM.from_pretrained(cp)\n",
    "tokenizer=AutoTokenizer.from_pretrained(cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9579701f-9e27-461a-bf9b-97ea0938e982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-66d144cf64d2204f\n",
      "Found cached dataset csv (/home/eldark/.cache/huggingface/datasets/csv/default-66d144cf64d2204f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47c908edb70f494987ccdad45a604101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-37505e917a9305ac\n",
      "Found cached dataset csv (/home/eldark/.cache/huggingface/datasets/csv/default-37505e917a9305ac/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d81fd9be7ec54636adae33fd0f42b629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "ds=datasets.load_dataset(\"csv\", data_files='train_df_no_niqqud.csv',sep=\"\\t\")\n",
    "dsv=datasets.load_dataset(\"csv\", data_files='valid_df_no_niqqud.csv',sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69ec7572-489f-48d7-9ac1-411fd90a7a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/eldark/.cache/huggingface/datasets/csv/default-66d144cf64d2204f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-33f023d48296fe59.arrow\n",
      "Loading cached processed dataset at /home/eldark/.cache/huggingface/datasets/csv/default-37505e917a9305ac/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-11db02a9408705f3.arrow\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(dataset):\n",
    "  return tokenizer(str(dataset[\"verse\"])\n",
    "                      )\n",
    "\n",
    "tokenized_ds=ds.map(tokenize_function)\n",
    "tokenized_dsv=dsv.map(tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "381c983c-9bae-435c-9fe0-f226e9470fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c6bed47-a3dc-4704-91da-d40d525f5ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(ds[\"train\"])\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"Models/Embibert-finetuned-distilBert-50-epochs\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=False,\n",
    "    logging_steps=logging_steps,\n",
    "    num_train_epochs=50,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55614b1e-87bf-44fc-9398-3d1e6cd3496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_ds['train'],\n",
    "    eval_dataset=tokenized_dsv,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4b1fd9c-50e6-4520-838a-135f2a5eeb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7aa3518-d0a4-4464-acef-e740460bfd52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the GPU available? True\n"
     ]
    }
   ],
   "source": [
    "gpu_avail = torch.cuda.is_available()\n",
    "print(f\"Is the GPU available? {gpu_avail}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8bd1b09-f733-43e5-96c0-29add16c611c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "/home/eldark/.local/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 22144\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 34600\n",
      "  Number of trainable parameters = 135445755\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34600' max='34600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [34600/34600 2:13:19, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.187200</td>\n",
       "      <td>2.969725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.842000</td>\n",
       "      <td>2.818247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.658400</td>\n",
       "      <td>2.741235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.562100</td>\n",
       "      <td>2.595029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.459900</td>\n",
       "      <td>2.505307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.386800</td>\n",
       "      <td>2.476544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.314600</td>\n",
       "      <td>2.450655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.249600</td>\n",
       "      <td>2.363181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.201800</td>\n",
       "      <td>2.370291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.146100</td>\n",
       "      <td>2.345382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.114200</td>\n",
       "      <td>2.402485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.065400</td>\n",
       "      <td>2.236766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.036500</td>\n",
       "      <td>2.260209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.964400</td>\n",
       "      <td>2.238271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.931800</td>\n",
       "      <td>2.183084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.898800</td>\n",
       "      <td>2.170995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.846000</td>\n",
       "      <td>2.277344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.823700</td>\n",
       "      <td>2.111136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.783600</td>\n",
       "      <td>2.217605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.761200</td>\n",
       "      <td>2.092140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.714300</td>\n",
       "      <td>2.181876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.670700</td>\n",
       "      <td>1.975282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.655000</td>\n",
       "      <td>2.014163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.614300</td>\n",
       "      <td>2.065641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.582900</td>\n",
       "      <td>2.117466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.539500</td>\n",
       "      <td>2.155001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.513000</td>\n",
       "      <td>2.030087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.491800</td>\n",
       "      <td>1.982562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.448800</td>\n",
       "      <td>1.996224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.432100</td>\n",
       "      <td>1.961962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.392600</td>\n",
       "      <td>2.069867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.367800</td>\n",
       "      <td>2.028589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.338400</td>\n",
       "      <td>2.009475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.293100</td>\n",
       "      <td>1.925828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.288200</td>\n",
       "      <td>1.945498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.259000</td>\n",
       "      <td>1.887161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.231700</td>\n",
       "      <td>1.987005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.208600</td>\n",
       "      <td>1.946332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.185900</td>\n",
       "      <td>2.039415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.151100</td>\n",
       "      <td>1.958746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.123900</td>\n",
       "      <td>2.076742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.100500</td>\n",
       "      <td>1.977366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.086900</td>\n",
       "      <td>2.050041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.061400</td>\n",
       "      <td>1.929709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.055900</td>\n",
       "      <td>2.022401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.033300</td>\n",
       "      <td>1.971170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.014100</td>\n",
       "      <td>1.954426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.004700</td>\n",
       "      <td>2.000667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.991900</td>\n",
       "      <td>1.937920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.991000</td>\n",
       "      <td>2.061171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-500\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-500/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-1000\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-1000/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-1500\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-1500/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-2000\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-2000/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-2500\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-2500/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-3000\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-3000/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-3500\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-3500/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-3500/special_tokens_map.json\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-4000\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-4000/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-4000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-4500\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-4500/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-4500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-5000\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-5000/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-5000/special_tokens_map.json\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-5500\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-5500/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-5500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-6000\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-6000/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-6000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-6500\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-6500/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-6500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-7000\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-7000/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-7000/special_tokens_map.json\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-7500\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-7500/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-7500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-8000\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-8000/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-8000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-8500\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-8500/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-8500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-9000\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-9000/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-9000/special_tokens_map.json\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-9500\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-9500/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-9500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-10000\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-10000/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-10000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-10500\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-10500/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-10500/special_tokens_map.json\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-11000\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-11000/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-11000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-11500\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-11500/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-11500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-12000\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-12000/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-12000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-12500\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-12500/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-12500/special_tokens_map.json\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-13000\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-13000/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-13000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-13500\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-13500/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-13500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-14000\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-14000/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-14000/special_tokens_map.json\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-14500\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-14500/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-14500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-15000\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-15000/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-15000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-15500\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-15500/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-15500/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-15500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-16000\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-16000/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-16000/special_tokens_map.json\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-16500\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-16500/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-16500/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-16500/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-16500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-17000\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-17000/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-17000/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-17000/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-17000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-17500\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-17500/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-17500/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-17500/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-17500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-18000\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-18000/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-18000/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-18000/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-18000/special_tokens_map.json\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-18500\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-18500/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-18500/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-18500/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-18500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-19000\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-19000/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-19000/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-19000/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-19000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-19500\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-19500/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-19500/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-19500/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-19500/special_tokens_map.json\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-20000\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-20000/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-20000/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-20000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-20500\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-20500/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-20500/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-20500/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-20500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-21000\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-21000/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-21000/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-21000/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-21000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-21500\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-21500/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-21500/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-21500/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-21500/special_tokens_map.json\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-22000\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-22000/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-22000/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-22000/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-22000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-22500\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-22500/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-22500/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-22500/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-22500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-23000\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-23000/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-23000/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-23000/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-23000/special_tokens_map.json\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-23500\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-23500/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-23500/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-23500/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-23500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-24000\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-24000/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-24000/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-24000/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-24000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-24500\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-24500/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-24500/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-24500/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-24500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-25000\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-25000/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-25000/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-25000/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-25000/special_tokens_map.json\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-25500\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-25500/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-25500/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-25500/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-25500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-26000\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-26000/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-26000/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-26000/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-26000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-26500\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-26500/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-26500/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-26500/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-26500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-27000\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-27000/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-27000/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-27000/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-27000/special_tokens_map.json\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-27500\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-27500/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-27500/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-27500/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-27500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-28000\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-28000/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-28000/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-28000/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-28000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-28500\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-28500/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-28500/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-28500/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-28500/special_tokens_map.json\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-29000\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-29000/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-29000/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-29000/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-29000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-29500\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-29500/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-29500/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-29500/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-29500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-30000\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-30000/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-30000/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-30000/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-30000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-30500\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-30500/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-30500/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-30500/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-30500/special_tokens_map.json\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-31000\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-31000/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-31000/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-31000/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-31000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-31500\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-31500/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-31500/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-31500/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-31500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-32000\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-32000/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-32000/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-32000/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-32000/special_tokens_map.json\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-32500\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-32500/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-32500/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-32500/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-32500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-33000\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-33000/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-33000/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-33000/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-33000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-33500\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-33500/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-33500/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-33500/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-33500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-34000\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-34000/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-34000/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-34000/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-34000/special_tokens_map.json\n",
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-34500\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-34500/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-34500/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-34500/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/checkpoint-34500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse. If name, Unnamed: 0, Unnamed: 0.1, verse_idx, verse are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=34600, training_loss=1.6615309645812635, metrics={'train_runtime': 8000.0456, 'train_samples_per_second': 138.399, 'train_steps_per_second': 4.325, 'total_flos': 1.7569739073371904e+16, 'train_loss': 1.6615309645812635, 'epoch': 50.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cfc65fc-49f7-49d5-acf8-afc743f8670a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Models/Embibert-finetuned-distilBert-50-epochs\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/pytorch_model.bin\n",
      "tokenizer config file saved in Models/Embibert-finetuned-distilBert-50-epochs/tokenizer_config.json\n",
      "Special tokens file saved in Models/Embibert-finetuned-distilBert-50-epochs/special_tokens_map.json\n",
      "Configuration saved in Models/Embibert-finetuned-distilBert-50-epochs/config.json\n",
      "Model weights saved in Models/Embibert-finetuned-distilBert-50-epochs/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"Models/Embibert-finetuned-distilBert-50-epochs\")\n",
    "model.save_pretrained(\"Models/Embibert-finetuned-distilBert-50-epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef3f876-9f3b-4b53-8050-35284d6f3401",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
