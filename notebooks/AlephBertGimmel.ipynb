{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898b79f2-33c0-4dcd-9908-7f23b6fd294d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1f3828f-a073-47d1-9d8e-18fb259b2472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import datasets\n",
    "import transformers\n",
    "from transformers import AutoTokenizer,AutoModelForMaskedLM\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd722330-aa59-4111-8bcf-9e59b9ddc844",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "601f21ab-df81-425d-b90a-2206528a3ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_csv('EmbibleDataset/train_df.csv', sep='\\\\t', encoding='utf-8')\n",
    "valid_df=pd.read_csv('EmbibleDataset/valid_df.csv', sep='\\\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b856dea-f6d6-4d11-96bd-11a7d5da20c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=train_df.drop(columns=[\"verse_idx.1\",\"verse.1\",\"name\"])\n",
    "#train_df[\"name.1\"].rename(\"name\")\n",
    "train_df.rename(columns={\"name.1\": 'name'}, inplace=True)\n",
    "#valid_df=train_df.drop(columns=[\"verse_idx.1\",\"verse.1\",\"name\"])\n",
    "#train_df[\"name.1\"].rename(\"name\")\n",
    "#valid_df.rename(columns={\"name.1\": 'name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b42299c-5c5e-40fb-a83a-a2dc8075f2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      verse_idx   verse                                               name\n",
      "0        Isaiah     0.0  חֲזוֹן֙ יְשַֽׁעְיָ֣הוּ בֶן אָמ֔וֹץ אֲשֶׁ֣ר חָז...\n",
      "1        Isaiah     1.0  שִׁמְע֤וּ שָׁמַ֙יִם֙ וְהַאֲזִ֣ינִי אֶ֔רֶץ כִּ֥...\n",
      "2        Isaiah     2.0  יָדַ֥ע שׁוֹר֙ קֹנֵ֔הוּ וַחֲמ֖וֹר אֵב֣וּס בְּעָ...\n",
      "3        Isaiah     3.0  ה֣וֹי גּ֣וֹי חֹטֵ֗א עַ֚ם כֶּ֣בֶד עָוֺ֔ן זֶ֣רַע...\n",
      "4        Isaiah     4.0  עַ֣ל מֶ֥ה תֻכּ֛וּ ע֖וֹד תּוֹסִ֣יפוּ סָרָ֑ה כָּ...\n",
      "...         ...     ...                                                ...\n",
      "22145    Exodus  1208.0  וַיְכַ֥ס הֶעָנָ֖ן אֶת אֹ֣הֶל מוֹעֵ֑ד וּכְב֣וֹד...\n",
      "22146    Exodus  1209.0  וְלֹא יָכֹ֣ל מֹשֶׁ֗ה לָבוֹא֙ אֶל אֹ֣הֶל מוֹעֵ֔...\n",
      "22147    Exodus  1210.0  וּבְהֵעָל֤וֹת הֶֽעָנָן֙ מֵעַ֣ל הַמִּשְׁכָּ֔ן י...\n",
      "22148    Exodus  1211.0  וְאִם לֹ֥א יֵעָלֶ֖ה הֶעָנָ֑ן וְלֹ֣א יִסְע֔וּ ע...\n",
      "22149    Exodus  1212.0  כִּי֩ עֲנַ֨ן יְהוָ֤ה עַֽל הַמִּשְׁכָּן֙ יוֹמָ֔...\n",
      "\n",
      "[22150 rows x 3 columns]\n",
      "         name  verse_idx                                              verse\n",
      "294    Isaiah      294.0  הָכִ֧ינוּ לְבָנָ֛יו מַטְבֵּ֖חַ בַּעֲוֺ֣ן אֲבוֹ...\n",
      "1252   Isaiah     1252.0  וְאַתֶּם֙ עֹזְבֵ֣י יְהוָ֔ה הַשְּׁכֵחִ֖ים אֶת ה...\n",
      "279    Isaiah      279.0  מַכֶּ֤ה עַמִּים֙ בְּעֶבְרָ֔ה מַכַּ֖ת בִּלְתִּ֣...\n",
      "541    Isaiah      541.0  וְחָנִ֥יתִי כַדּ֖וּר עָלָ֑יִךְ וְצַרְתִּ֤י עָל...\n",
      "619    Isaiah      619.0  עַד יֵ֨עָרֶ֥ה עָלֵ֛ינוּ ר֖וּחַ מִמָּר֑וֹם וְהָ...\n",
      "...       ...        ...                                                ...\n",
      "22964  Exodus      955.0  וַיֹּ֖אמֶר אֵלָ֑יו אִם אֵ֤ין פָּנֶ֙יךָ֙ הֹלְכִ...\n",
      "22263  Exodus      254.0  וַיֹּ֣אמֶר אֲלֵהֶ֗ם יְהִ֨י כֵ֤ן יְהוָה֙ עִמָּכ...\n",
      "23156  Exodus     1147.0  וַֽיַּעֲשׂ֗וּ שְׁתֵּי֙ מִשְׁבְּצֹ֣ת זָהָ֔ב וּש...\n",
      "22519  Exodus      510.0  וַיּוֹצֵ֨א מֹשֶׁ֧ה אֶת הָעָ֛ם לִקְרַ֥את הָֽאֱל...\n",
      "23207  Exodus     1198.0  וַיָּ֤שֶׂם אֶת הַמְּנֹרָה֙ בְּאֹ֣הֶל מוֹעֵ֔ד נ...\n",
      "\n",
      "[536 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_df)\n",
    "print(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fc7ee08-7ad8-4393-8c11-bdf652527560",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = 'onlplab/alephbert-base'\n",
    "\n",
    "model=AutoModelForMaskedLM.from_pretrained(cp)\n",
    "tokenizer=AutoTokenizer.from_pretrained(cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06406f33-d5a4-4426-9615-4110819fc44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds=datasets.Dataset.from_pandas(train_df)\n",
    "valid_ds=datasets.Dataset.from_pandas(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eaf4e82-d2b5-431d-b28a-756750c61dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "954a96628ead4b0d95ded852ceaf2d3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22150 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be7ccce699df4bdfb73099e991f36af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/536 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def tokenize_function(dataset):\n",
    "  return tokenizer(str(dataset[\"verse\"]))\n",
    "tokenized_train_ds=train_ds.map(tokenize_function)\n",
    "tokenized_valid_ds=valid_ds.map(tokenize_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa4d01f7-ef3f-4498-8cb6-4cd2afaaee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "984b3216-39f1-42b5-b142-b41895e8eb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(tokenized_train_ds)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=False,\n",
    "    logging_steps=logging_steps,\n",
    "    num_train_epochs=10\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "641f6ebb-61ff-4155-b1a8-ddfc9af924d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2d033a6-8ae9-48d7-8c9a-f485247e2ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_train_ds,\n",
    "    eval_dataset=tokenized_valid_ds,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8832463b-1443-40de-a72a-ded6b6993649",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: verse_idx, name, verse. If verse_idx, name, verse are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 22150\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6930\n",
      "  Number of trainable parameters = 126030112\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6930' max='6930' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6930/6930 14:59, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.734000</td>\n",
       "      <td>6.200029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.707500</td>\n",
       "      <td>7.757162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.699100</td>\n",
       "      <td>10.060718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.675400</td>\n",
       "      <td>10.159719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.658800</td>\n",
       "      <td>10.874681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.666500</td>\n",
       "      <td>12.445745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.630400</td>\n",
       "      <td>12.339250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.659600</td>\n",
       "      <td>13.200126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.643900</td>\n",
       "      <td>12.449395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.663800</td>\n",
       "      <td>12.418348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse_idx, name, verse. If __index_level_0__, verse_idx, name, verse are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-1000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-1000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse_idx, name, verse. If __index_level_0__, verse_idx, name, verse are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-1500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-1500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-2000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-2000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse_idx, name, verse. If __index_level_0__, verse_idx, name, verse are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-2500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-2500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse_idx, name, verse. If __index_level_0__, verse_idx, name, verse are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-3000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-3000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse_idx, name, verse. If __index_level_0__, verse_idx, name, verse are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-3500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-3500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-3500/special_tokens_map.json\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-4000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-4000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-4000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse_idx, name, verse. If __index_level_0__, verse_idx, name, verse are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-4500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-4500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-4500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse_idx, name, verse. If __index_level_0__, verse_idx, name, verse are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-5000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-5000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-5000/special_tokens_map.json\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-5500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-5500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-5500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse_idx, name, verse. If __index_level_0__, verse_idx, name, verse are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-6000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-6000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-6000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse_idx, name, verse. If __index_level_0__, verse_idx, name, verse are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-6500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-6500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/checkpoint-6500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse_idx, name, verse. If __index_level_0__, verse_idx, name, verse are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6930, training_loss=1.6739039565577651, metrics={'train_runtime': 900.3576, 'train_samples_per_second': 246.013, 'train_steps_per_second': 7.697, 'total_flos': 683372820672000.0, 'train_loss': 1.6739039565577651, 'epoch': 10.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a28e2523-e99b-4d76-b772-ec8d4a17923b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/special_tokens_map.json\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs\")\n",
    "model.save_pretrained(\"EmbibleModels/Embibert-finetuned-AlephBertGimmel-10epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45eab85b-1bad-4b80-8cbc-bb08b3d8f2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/fono/.cache/huggingface/hub/models--onlplab--alephbert-base/snapshots/1745fb3ff5137e41e9eb4d6246e0758f63b93e46/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"onlplab/alephbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/fono/.cache/huggingface/hub/models--onlplab--alephbert-base/snapshots/1745fb3ff5137e41e9eb4d6246e0758f63b93e46/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at onlplab/alephbert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "loading configuration file config.json from cache at /home/fono/.cache/huggingface/hub/models--onlplab--alephbert-base/snapshots/1745fb3ff5137e41e9eb4d6246e0758f63b93e46/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"onlplab/alephbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/fono/.cache/huggingface/hub/models--onlplab--alephbert-base/snapshots/1745fb3ff5137e41e9eb4d6246e0758f63b93e46/vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/fono/.cache/huggingface/hub/models--onlplab--alephbert-base/snapshots/1745fb3ff5137e41e9eb4d6246e0758f63b93e46/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/fono/.cache/huggingface/hub/models--onlplab--alephbert-base/snapshots/1745fb3ff5137e41e9eb4d6246e0758f63b93e46/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/fono/.cache/huggingface/hub/models--onlplab--alephbert-base/snapshots/1745fb3ff5137e41e9eb4d6246e0758f63b93e46/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"onlplab/alephbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/fono/.cache/huggingface/hub/models--onlplab--alephbert-base/snapshots/1745fb3ff5137e41e9eb4d6246e0758f63b93e46/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"onlplab/alephbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cp = 'onlplab/alephbert-base'\n",
    "model=AutoModelForMaskedLM.from_pretrained(cp)\n",
    "tokenizer=AutoTokenizer.from_pretrained(cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf27cf54-c402-4bad-90f3-35c85a5ab3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(tokenized_train_ds)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"EmbibleModels/Embibert-finetuned-AlephBertGimmel-20epochs\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=False,\n",
    "    logging_steps=logging_steps,\n",
    "    num_train_epochs=20\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "335427dc-c4c4-43e1-a1ca-4ed15e85d315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b560918c-0629-458b-b472-d446ecb81e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: verse_idx, name, verse. If verse_idx, name, verse are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 22150\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 13860\n",
      "  Number of trainable parameters = 126030112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('disk I/O error')).History will not be written to the database.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3501' max='13860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 3501/13860 07:11 < 21:16, 8.11 it/s, Epoch 5.05/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.663200</td>\n",
       "      <td>16.870060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.677900</td>\n",
       "      <td>18.831984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.675400</td>\n",
       "      <td>19.834837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.661600</td>\n",
       "      <td>16.043135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.658300</td>\n",
       "      <td>17.664694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-AlephBertGimmel-20epochs/checkpoint-500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-20epochs/checkpoint-500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-20epochs/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-20epochs/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-20epochs/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse_idx, name, verse. If __index_level_0__, verse_idx, name, verse are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-AlephBertGimmel-20epochs/checkpoint-1000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-20epochs/checkpoint-1000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-20epochs/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-20epochs/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-20epochs/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse_idx, name, verse. If __index_level_0__, verse_idx, name, verse are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-AlephBertGimmel-20epochs/checkpoint-1500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-20epochs/checkpoint-1500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-20epochs/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-20epochs/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-20epochs/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-AlephBertGimmel-20epochs/checkpoint-2000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-20epochs/checkpoint-2000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-20epochs/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-20epochs/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-20epochs/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse_idx, name, verse. If __index_level_0__, verse_idx, name, verse are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-AlephBertGimmel-20epochs/checkpoint-2500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-20epochs/checkpoint-2500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-20epochs/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-20epochs/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-20epochs/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse_idx, name, verse. If __index_level_0__, verse_idx, name, verse are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-AlephBertGimmel-20epochs/checkpoint-3000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-20epochs/checkpoint-3000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-20epochs/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-20epochs/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-20epochs/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse_idx, name, verse. If __index_level_0__, verse_idx, name, verse are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-AlephBertGimmel-20epochs/checkpoint-3500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-20epochs/checkpoint-3500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-20epochs/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-20epochs/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-AlephBertGimmel-20epochs/checkpoint-3500/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_train_ds,\n",
    "    eval_dataset=tokenized_valid_ds,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aabcc93-1279-4914-a110-7ba7731ad023",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"EmbibleModels/Embibert-finetuned-AlephBertGimmel-20epochs\")\n",
    "model.save_pretrained(\"EmbibleModels/Embibert-finetuned-AlephBertGimmel-20epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f411f07a-1db8-4555-8804-1f943b7acae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = 'onlplab/alephbert-base'\n",
    "model=AutoModelForMaskedLM.from_pretrained(cp)\n",
    "tokenizer=AutoTokenizer.from_pretrained(cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e96b6b-eff5-4542-abea-e3cc5ba61855",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(tokenized_train_ds)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"EmbibleModels/Embibert-finetuned-AlephBertGimmel-50epochs\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=False,\n",
    "    logging_steps=logging_steps,\n",
    "    num_train_epochs=50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291da944-ddc9-4128-a2cb-2fd6bb02758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dfcc52-2df2-453b-8df7-924d11958675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_train_ds,\n",
    "    eval_dataset=tokenized_valid_ds,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac154950-443b-470b-8384-9b039fbb977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"EmbibleModels/Embibert-finetuned-AlephBertGimmel-50epochs\")\n",
    "model.save_pretrained(\"EmbibleModels/Embibert-finetuned-AlephBertGimmel-50epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda9fc54-0ef8-4e81-90e1-e14cfb991870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a4a0ca-6f47-477d-a4db-255838e33c35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
