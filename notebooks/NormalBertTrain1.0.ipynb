{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898b79f2-33c0-4dcd-9908-7f23b6fd294d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1f3828f-a073-47d1-9d8e-18fb259b2472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import datasets\n",
    "import transformers\n",
    "from transformers import AutoTokenizer,AutoModelForMaskedLM\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "import warnings\n",
    "from transformers import BertTokenizer, BertModel\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd722330-aa59-4111-8bcf-9e59b9ddc844",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "601f21ab-df81-425d-b90a-2206528a3ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_csv('EmbibleDataset/train_df.csv', sep='\\\\t', encoding='utf-8')\n",
    "valid_df=pd.read_csv('EmbibleDataset/valid_df.csv', sep='\\\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b856dea-f6d6-4d11-96bd-11a7d5da20c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=train_df.drop(columns=[\"verse_idx.1\",\"verse.1\",\"name\"])\n",
    "#train_df[\"name.1\"].rename(\"name\")\n",
    "train_df.rename(columns={\"name.1\": 'name'}, inplace=True)\n",
    "#valid_df=train_df.drop(columns=[\"verse_idx.1\",\"verse.1\",\"name\"])\n",
    "#train_df[\"name.1\"].rename(\"name\")\n",
    "#valid_df.rename(columns={\"name.1\": 'name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b42299c-5c5e-40fb-a83a-a2dc8075f2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      verse_idx   verse                                               name\n",
      "0        Isaiah     0.0  חֲזוֹן֙ יְשַֽׁעְיָ֣הוּ בֶן אָמ֔וֹץ אֲשֶׁ֣ר חָז...\n",
      "1        Isaiah     1.0  שִׁמְע֤וּ שָׁמַ֙יִם֙ וְהַאֲזִ֣ינִי אֶ֔רֶץ כִּ֥...\n",
      "2        Isaiah     2.0  יָדַ֥ע שׁוֹר֙ קֹנֵ֔הוּ וַחֲמ֖וֹר אֵב֣וּס בְּעָ...\n",
      "3        Isaiah     3.0  ה֣וֹי גּ֣וֹי חֹטֵ֗א עַ֚ם כֶּ֣בֶד עָוֺ֔ן זֶ֣רַע...\n",
      "4        Isaiah     4.0  עַ֣ל מֶ֥ה תֻכּ֛וּ ע֖וֹד תּוֹסִ֣יפוּ סָרָ֑ה כָּ...\n",
      "...         ...     ...                                                ...\n",
      "22145    Exodus  1208.0  וַיְכַ֥ס הֶעָנָ֖ן אֶת אֹ֣הֶל מוֹעֵ֑ד וּכְב֣וֹד...\n",
      "22146    Exodus  1209.0  וְלֹא יָכֹ֣ל מֹשֶׁ֗ה לָבוֹא֙ אֶל אֹ֣הֶל מוֹעֵ֔...\n",
      "22147    Exodus  1210.0  וּבְהֵעָל֤וֹת הֶֽעָנָן֙ מֵעַ֣ל הַמִּשְׁכָּ֔ן י...\n",
      "22148    Exodus  1211.0  וְאִם לֹ֥א יֵעָלֶ֖ה הֶעָנָ֑ן וְלֹ֣א יִסְע֔וּ ע...\n",
      "22149    Exodus  1212.0  כִּי֩ עֲנַ֨ן יְהוָ֤ה עַֽל הַמִּשְׁכָּן֙ יוֹמָ֔...\n",
      "\n",
      "[22150 rows x 3 columns]\n",
      "         name  verse_idx                                              verse\n",
      "294    Isaiah      294.0  הָכִ֧ינוּ לְבָנָ֛יו מַטְבֵּ֖חַ בַּעֲוֺ֣ן אֲבוֹ...\n",
      "1252   Isaiah     1252.0  וְאַתֶּם֙ עֹזְבֵ֣י יְהוָ֔ה הַשְּׁכֵחִ֖ים אֶת ה...\n",
      "279    Isaiah      279.0  מַכֶּ֤ה עַמִּים֙ בְּעֶבְרָ֔ה מַכַּ֖ת בִּלְתִּ֣...\n",
      "541    Isaiah      541.0  וְחָנִ֥יתִי כַדּ֖וּר עָלָ֑יִךְ וְצַרְתִּ֤י עָל...\n",
      "619    Isaiah      619.0  עַד יֵ֨עָרֶ֥ה עָלֵ֛ינוּ ר֖וּחַ מִמָּר֑וֹם וְהָ...\n",
      "...       ...        ...                                                ...\n",
      "22964  Exodus      955.0  וַיֹּ֖אמֶר אֵלָ֑יו אִם אֵ֤ין פָּנֶ֙יךָ֙ הֹלְכִ...\n",
      "22263  Exodus      254.0  וַיֹּ֣אמֶר אֲלֵהֶ֗ם יְהִ֨י כֵ֤ן יְהוָה֙ עִמָּכ...\n",
      "23156  Exodus     1147.0  וַֽיַּעֲשׂ֗וּ שְׁתֵּי֙ מִשְׁבְּצֹ֣ת זָהָ֔ב וּש...\n",
      "22519  Exodus      510.0  וַיּוֹצֵ֨א מֹשֶׁ֧ה אֶת הָעָ֛ם לִקְרַ֥את הָֽאֱל...\n",
      "23207  Exodus     1198.0  וַיָּ֤שֶׂם אֶת הַמְּנֹרָה֙ בְּאֹ֣הֶל מוֹעֵ֔ד נ...\n",
      "\n",
      "[536 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_df)\n",
    "print(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fc7ee08-7ad8-4393-8c11-bdf652527560",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/fono/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/800c34f3d5aa174fe531f560b44b8d14592225b7/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 105879\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/fono/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/800c34f3d5aa174fe531f560b44b8d14592225b7/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at bert-base-multilingual-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "loading configuration file config.json from cache at /home/fono/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/800c34f3d5aa174fe531f560b44b8d14592225b7/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 105879\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/fono/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/800c34f3d5aa174fe531f560b44b8d14592225b7/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/fono/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/800c34f3d5aa174fe531f560b44b8d14592225b7/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/fono/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/800c34f3d5aa174fe531f560b44b8d14592225b7/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/fono/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/800c34f3d5aa174fe531f560b44b8d14592225b7/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 105879\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "# model = BertModel.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "\n",
    "# cp = \"tau/tavbert-he\"\n",
    "model=AutoModelForMaskedLM.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "tokenizer=AutoTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06406f33-d5a4-4426-9615-4110819fc44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds=datasets.Dataset.from_pandas(train_df)\n",
    "valid_ds=datasets.Dataset.from_pandas(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0eaf4e82-d2b5-431d-b28a-756750c61dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72733c0fa0a043729f05b9ee33791ece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22150 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94cb9a4fe14d46d89fb1a6abb3e26a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/536 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def tokenize_function(dataset):\n",
    "  return tokenizer(str(dataset[\"verse\"]))\n",
    "tokenized_train_ds=train_ds.map(tokenize_function)\n",
    "tokenized_valid_ds=valid_ds.map(tokenize_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa4d01f7-ef3f-4498-8cb6-4cd2afaaee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "984b3216-39f1-42b5-b142-b41895e8eb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(tokenized_train_ds)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"EmbibleModels/Embibert-finetuned-NormalBert-10epochs\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=False,\n",
    "    num_train_epochs=10\n",
    ")\n",
    "#training_args = TrainingArguments(\"test-trainer\",num_train_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "641f6ebb-61ff-4155-b1a8-ddfc9af924d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import f1_score\n",
    "\n",
    "# def compute_metrics(pred):\n",
    "#   return {'f1:':pred}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2d033a6-8ae9-48d7-8c9a-f485247e2ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_train_ds,\n",
    "    eval_dataset=tokenized_valid_ds,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81e64183-cc50-4e3c-a601-296c98bee544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: verse, name, verse_idx. If verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 22150\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3470\n",
      "  Number of trainable parameters = 167463831\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3470' max='3470' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3470/3470 11:12, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.150000</td>\n",
       "      <td>5.984001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.119500</td>\n",
       "      <td>6.764743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.135600</td>\n",
       "      <td>8.432836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.057600</td>\n",
       "      <td>8.031181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.158200</td>\n",
       "      <td>8.931014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.138600</td>\n",
       "      <td>9.223641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.128300</td>\n",
       "      <td>8.655166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.075500</td>\n",
       "      <td>9.058912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.107500</td>\n",
       "      <td>8.816860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.136200</td>\n",
       "      <td>9.040538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-10epochs/checkpoint-500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-10epochs/checkpoint-500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-10epochs/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-10epochs/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-10epochs/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-10epochs/checkpoint-1000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-10epochs/checkpoint-1000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-10epochs/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-10epochs/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-10epochs/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-10epochs/checkpoint-1500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-10epochs/checkpoint-1500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-10epochs/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-10epochs/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-10epochs/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-10epochs/checkpoint-2000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-10epochs/checkpoint-2000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-10epochs/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-10epochs/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-10epochs/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-10epochs/checkpoint-2500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-10epochs/checkpoint-2500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-10epochs/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-10epochs/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-10epochs/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-10epochs/checkpoint-3000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-10epochs/checkpoint-3000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-10epochs/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-10epochs/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-10epochs/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3470, training_loss=2.120715068190517, metrics={'train_runtime': 672.5892, 'train_samples_per_second': 329.324, 'train_steps_per_second': 5.159, 'total_flos': 682822798042032.0, 'train_loss': 2.120715068190517, 'epoch': 10.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eb2e20-ed58-42bf-b03b-8df66f03e23d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38887099-b67e-42a6-9dd0-4ed6ec95d9ab",
   "metadata": {},
   "source": [
    "before training 10.36 after 9.37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b5d385-19ec-4f69-a0fc-1b27bd9152cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a28e2523-e99b-4d76-b772-ec8d4a17923b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-10epochs\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-10epochs/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-10epochs/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-10epochs/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-10epochs/special_tokens_map.json\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-10epochs/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-10epochs/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"EmbibleModels/Embibert-finetuned-NormalBert-10epochs\")\n",
    "model.save_pretrained(\"EmbibleModels/Embibert-finetuned-NormalBert-10epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45eab85b-1bad-4b80-8cbc-bb08b3d8f2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/fono/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/800c34f3d5aa174fe531f560b44b8d14592225b7/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 105879\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/fono/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/800c34f3d5aa174fe531f560b44b8d14592225b7/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at bert-base-multilingual-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "loading configuration file config.json from cache at /home/fono/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/800c34f3d5aa174fe531f560b44b8d14592225b7/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 105879\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/fono/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/800c34f3d5aa174fe531f560b44b8d14592225b7/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/fono/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/800c34f3d5aa174fe531f560b44b8d14592225b7/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/fono/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/800c34f3d5aa174fe531f560b44b8d14592225b7/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/fono/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/800c34f3d5aa174fe531f560b44b8d14592225b7/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 105879\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=AutoModelForMaskedLM.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "tokenizer=AutoTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe515c5c-5deb-4a40-8364-3208305daddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(tokenized_train_ds)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"EmbibleModels/Embibert-finetuned-NormalBert-20epochs\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=False,\n",
    "    num_train_epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6ace9ee-0649-4766-8e95-6577b52e5a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: verse, name, verse_idx. If verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 22150\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6940\n",
      "  Number of trainable parameters = 167463831\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6940' max='6940' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6940/6940 23:03, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.150500</td>\n",
       "      <td>6.051132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.118300</td>\n",
       "      <td>7.462174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.136600</td>\n",
       "      <td>8.595393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.059900</td>\n",
       "      <td>8.554097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.159100</td>\n",
       "      <td>10.103720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.144900</td>\n",
       "      <td>10.146825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.133200</td>\n",
       "      <td>10.348342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.080300</td>\n",
       "      <td>10.605648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.113700</td>\n",
       "      <td>10.596652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.140400</td>\n",
       "      <td>11.716318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.093800</td>\n",
       "      <td>11.045471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.094700</td>\n",
       "      <td>10.653270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.125700</td>\n",
       "      <td>10.961277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.090800</td>\n",
       "      <td>10.707878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.077700</td>\n",
       "      <td>10.991650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.131700</td>\n",
       "      <td>10.859967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.077300</td>\n",
       "      <td>10.986838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.094500</td>\n",
       "      <td>11.053352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.058200</td>\n",
       "      <td>11.106441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.081700</td>\n",
       "      <td>11.050654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-1000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-1000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-1500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-1500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-2000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-2000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-2500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-2500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-3000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-3000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-3500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-3500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-3500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-4000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-4000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-4000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-4500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-4500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-4500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-5000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-5000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-5000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-5500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-5500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-5500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-6000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-6000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-6000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-6500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-6500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/checkpoint-6500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6940, training_loss=2.108155325853859, metrics={'train_runtime': 1384.0366, 'train_samples_per_second': 320.078, 'train_steps_per_second': 5.014, 'total_flos': 1365385246918812.0, 'train_loss': 2.108155325853859, 'epoch': 20.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_train_ds,\n",
    "    eval_dataset=tokenized_valid_ds,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1c6d90c5-aba6-4560-b01e-08aad7566e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-20epochs\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/special_tokens_map.json\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-20epochs/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"EmbibleModels/Embibert-finetuned-NormalBert-20epochs\")\n",
    "model.save_pretrained(\"EmbibleModels/Embibert-finetuned-NormalBert-20epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7345c2a6-f76a-418c-9ed9-dd1176d19b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/fono/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/800c34f3d5aa174fe531f560b44b8d14592225b7/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 105879\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/fono/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/800c34f3d5aa174fe531f560b44b8d14592225b7/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at bert-base-multilingual-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "loading configuration file config.json from cache at /home/fono/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/800c34f3d5aa174fe531f560b44b8d14592225b7/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 105879\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/fono/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/800c34f3d5aa174fe531f560b44b8d14592225b7/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/fono/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/800c34f3d5aa174fe531f560b44b8d14592225b7/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/fono/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/800c34f3d5aa174fe531f560b44b8d14592225b7/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/fono/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/800c34f3d5aa174fe531f560b44b8d14592225b7/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 105879\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=AutoModelForMaskedLM.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "tokenizer=AutoTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "77529cba-1183-4358-8704-e024619bd3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(tokenized_train_ds)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"EmbibleModels/Embibert-finetuned-NormalBert-50epochs\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=False,\n",
    "    num_train_epochs=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4139f263-c236-42b5-93d3-f1269e482d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: verse, name, verse_idx. If verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 22150\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 17350\n",
      "  Number of trainable parameters = 167463831\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17350' max='17350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17350/17350 1:09:23, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.150500</td>\n",
       "      <td>6.225634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.118800</td>\n",
       "      <td>7.624661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.140200</td>\n",
       "      <td>8.652073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.061900</td>\n",
       "      <td>8.300903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.161500</td>\n",
       "      <td>9.615662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.145200</td>\n",
       "      <td>9.923818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.141900</td>\n",
       "      <td>10.786325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.084600</td>\n",
       "      <td>10.838971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.116000</td>\n",
       "      <td>11.285677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.144400</td>\n",
       "      <td>12.821379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.098400</td>\n",
       "      <td>12.184176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.097800</td>\n",
       "      <td>11.778652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.129900</td>\n",
       "      <td>11.974170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.096000</td>\n",
       "      <td>12.398367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.083200</td>\n",
       "      <td>12.491139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.139500</td>\n",
       "      <td>13.433257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.084000</td>\n",
       "      <td>13.225143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.102100</td>\n",
       "      <td>13.800085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.064000</td>\n",
       "      <td>14.120675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.087900</td>\n",
       "      <td>13.494293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.105000</td>\n",
       "      <td>14.712826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.134200</td>\n",
       "      <td>14.853824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.059800</td>\n",
       "      <td>14.963495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.053100</td>\n",
       "      <td>15.044737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.065200</td>\n",
       "      <td>13.890547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.106100</td>\n",
       "      <td>15.425363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2.105000</td>\n",
       "      <td>15.089802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.091100</td>\n",
       "      <td>15.124846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>2.106500</td>\n",
       "      <td>15.285462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.116900</td>\n",
       "      <td>14.976112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2.113900</td>\n",
       "      <td>15.172292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2.093300</td>\n",
       "      <td>14.723693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.111000</td>\n",
       "      <td>14.829755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2.102900</td>\n",
       "      <td>15.014249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.102600</td>\n",
       "      <td>15.547217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>2.089600</td>\n",
       "      <td>15.569230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>2.007700</td>\n",
       "      <td>15.922733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>2.057000</td>\n",
       "      <td>15.338756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>2.098500</td>\n",
       "      <td>16.275547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.094000</td>\n",
       "      <td>16.147314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>2.092900</td>\n",
       "      <td>16.093506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>2.080700</td>\n",
       "      <td>16.165010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>2.095200</td>\n",
       "      <td>16.352194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>2.064000</td>\n",
       "      <td>16.165468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>2.086400</td>\n",
       "      <td>15.956669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>2.030100</td>\n",
       "      <td>15.930452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>2.059200</td>\n",
       "      <td>16.031786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>2.088600</td>\n",
       "      <td>15.786323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>2.071300</td>\n",
       "      <td>15.878108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.063800</td>\n",
       "      <td>15.930412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-1000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-1000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-1500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-1500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-2000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-2000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-2500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-2500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-3000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-3000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-3500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-3500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-3500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-4000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-4000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-4000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-4500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-4500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-4500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-5000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-5000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-5000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-5500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-5500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-5500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-6000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-6000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-6000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-6500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-6500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-6500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-7000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-7000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-7000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-7500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-7500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-7500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-8000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-8000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-8000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-8500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-8500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-8500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-9000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-9000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-9000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-9500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-9500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-9500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-10000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-10000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-10000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-10500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-10500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-10500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-11000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-11000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-11000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-11500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-11500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-11500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-12000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-12000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-12000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-12500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-12500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-12500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-13000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-13000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-13000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-13500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-13500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-13500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-14000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-14000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-14000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-14500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-14500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-14500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-15000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-15000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-15000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-15500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-15500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-15500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-15500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-16000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-16000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-16000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-16500\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-16500/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-16500/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-16500/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-16500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-17000\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-17000/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-17000/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-17000/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/checkpoint-17000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, verse, name, verse_idx. If __index_level_0__, verse, name, verse_idx are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 536\n",
      "  Batch size = 64\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=17350, training_loss=2.0958767533096183, metrics={'train_runtime': 4163.9206, 'train_samples_per_second': 265.975, 'train_steps_per_second': 4.167, 'total_flos': 3413345291291412.0, 'train_loss': 2.0958767533096183, 'epoch': 50.0})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_train_ds,\n",
    "    eval_dataset=tokenized_valid_ds,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7691e3e7-b024-4505-9ef1-4efb5d4fef74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to EmbibleModels/Embibert-finetuned-NormalBert-50epochs\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/pytorch_model.bin\n",
      "tokenizer config file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/tokenizer_config.json\n",
      "Special tokens file saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/special_tokens_map.json\n",
      "Configuration saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/config.json\n",
      "Model weights saved in EmbibleModels/Embibert-finetuned-NormalBert-50epochs/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"EmbibleModels/Embibert-finetuned-NormalBert-50epochs\")\n",
    "model.save_pretrained(\"EmbibleModels/Embibert-finetuned-NormalBert-50epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fdd833-cb86-4fbe-b88a-8c9fa30a0e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
