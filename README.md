# Embible

## Analyze The Results For Fine-Tuning To several Bert models.

We trained all the models for: 10,20 and 50 epochs with trying different parameters: batch size, learning rate, weight decay

<!-- TABLE_GENERATE_START -->

| First Header  | Second Header | Second Header |Second Header |Second Header |
| ------------- | ------------- |------------- |------------- |-------------  |
| DistilBert  | 10  |0.0002  |32  |0.01    |
| DistilBert  | 20  |0.0002  |32  |0.01    |
| DistilBert  | 50  |0.0002  |32  |0.01    |
| Mbert | 10  |0.000002  |16  |0.1   |
| Mbert  | 20  |0.000002  |16  |0.1   |
| Mbert  | 50  |0.000002  |16  |0.1  |
| tavBert  | 10  |0.000002  |32  |0.01   |
| tavBert  | 20  |0.000002  |32  |0.01   |
| tavBert  | 50  |0.000002  |32  |0.01   |
| Aleph-Bert-gimel  | 10  |0.000002  |16 |0.01  |
| Aleph-Bert-gimel  | 20  |0.000002  |16  |0.01   |
| Aleph-Bert-gimel  | 50 |0.000002  |16  |0.01  |


<!-- TABLE_GENERATE_END -->

